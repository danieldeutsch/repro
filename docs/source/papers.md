The following papers have models implemented in Repro.

## Summarization
|Paper|Authors|Docs|
|-|-|-|
|[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)|Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer|[Link](lewis2020.md)|
|[Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345)|Yang Liu and Mirella Lapata|[Link](liu2019.md)|
|[GSum: A General Framework for Guided Neural Abstractive Summarization](https://arxiv.org/abs/2010.08014)|Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig|[Link](dou2021.md)|

## Text Generation Evaluation
|Paper|Authors|Docs|
|-|-|-|
|[ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013/)|Chin-Yew Lin|[Link](sacrerouge.md)|
|[Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary](https://arxiv.org/abs/2010.00490)|Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth|[Link](deutsch2021.md)|
|[BLEURT: Learning Robust Metrics for Text Generation](https://arxiv.org/abs/2004.04696)|Thibault Sellam, Dipanjan Das, and Ankur P. Parikh|[Link](sellam2020.md)|
|[BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)|Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi|[Link](zhang2020.md)|
|[BLEU: A Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040/)|Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu|[Link](papineni2002.md)|
|[QuestEval: Summarization Asks for Fact-based Evaluation](https://arxiv.org/abs/2103.12693)|Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang|[Link](scialom2021.md)|
|[MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance](https://aclanthology.org/D19-1053/)|Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger|[Link](zhao2019.md)|
|[FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization](https://aclanthology.org/2020.acl-main.454/)|Esin Durmus, He He, and Mona Diab|[Link](durmus2020.md)|
|[Evaluating Factuality in Generation with Dependency-level Entailment](https://aclanthology.org/2020.findings-emnlp.322/)|Tanya Goyal and Greg Durrett|[Link](goyal2020.md)|
|[Evaluating the Factual Consistency of Abstractive Text Summarization](https://arxiv.org/abs/1910.12840)|Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher|[Link](kryscinski2019.md)|
|[Answers Unite! Unsupervised Metrics for Reinforced Summarization Models](https://arxiv.org/abs/1909.01610)|Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano|[Link](scialom2019.md)|
|[NUBIA: NeUral Based Interchangeability Assessor for Text Generation](https://arxiv.org/abs/2004.14667)|Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh, and Mohamed Coulibali|[Link](kane2020.md)|
|[Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](https://arxiv.org/abs/2004.14564)|Brian Thompson and Matt Post|[Link](thompson2020.md)|
|[Finding a Balanced Degree of Automation for Summary Evaluation](https://arxiv.org/abs/2109.11503)|Shiyue Zhang and Mohit Bansal|[Link](zhang2021.md)|
|[BARTScore: Evaluating Generated Text as Text Generation](https://arxiv.org/abs/2106.11520)|Weizhe Yuan, Graham Neubig, and Pengfei Liu|[Link](yuan2021.md)|
|[CLIPScore: A Reference-free Evaluation Metric for Image Captioning](https://arxiv.org/abs/2104.08718)|Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi|[Link](hessel2021.md)|
|[SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization](https://arxiv.org/abs/2005.03724)|Yang Gao, Wei Zhao, and Steffen Eger|[Link](gao2020.md)|
|[Fill in the BLANC: Human-free quality estimation of document summaries](https://aclanthology.org/2020.eval4nlp-1.2/)|Oleg Vasilyev, Vedant Dharnidharka, and John Bohannon|[Link](vasilyev2020.md)|
|[Meteor Universal: Language Specific Translation Evaluation for Any Target Language](https://aclanthology.org/W14-3348/)|Michael Denkowski and Alon Lavie|[Link](denkowski2014.md)|
|[COMET: A Neural Framework for MT Evaluation](https://aclanthology.org/2020.emnlp-main.213/)|Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie|[Link](rei2020.md)|
|[Automatic Text Evaluation through the Lens of Wasserstein Barycenters](https://arxiv.org/abs/2108.12463)|Pierre Colombo, Guillaume Staerman, Chloe Clavel, and Pablo Piantanida|[Link](colombo2021.md)|
|[InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation](https://arxiv.org/abs/2112.01589)|Pierre Colombo, Chloe Clavel, and Pablo Piantanida|[Link](colombo2021.md)|
|[A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions](https://arxiv.org/abs/2103.12711)|Guillaume Staerman, Pavlo Mozharovskyi, Pierre Colombo, Stéphan Clémençon, and Florence d'Alché-Buc|[Link](colombo2021.md)|
|[Just Ask! Evaluating Machine Translation by Asking and Answering Questions](https://www.statmt.org/wmt21/pdf/2021.wmt-1.58.pdf)|Mateusz Krubinski, Erfan Ghadery, Marie-Francine Moens, Pavel Pecina|[Link](krubinski2021.md)|

## Question Answering
|Paper|Authors|Docs|
|-|-|-|
|[Neural Module Networks for Reasoning over Text](https://arxiv.org/abs/1912.04971)|Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner|[Link](gupta2020.md)|
|[Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary](https://arxiv.org/abs/2010.00490)|Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth|[Link](deutsch2021.md)|

## Question Generation
|Paper|Authors|Docs|
|-|-|-|
|[Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary](https://arxiv.org/abs/2010.00490)|Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth|[Link](deutsch2021.md)|
|[Asking It All: Generating Contextualized Questions for any Semantic Role](https://arxiv.org/abs/2109.04832)|Valentina Pyatkin, Paul Roit, Julian Michael, Reut Tsarfaty, Yoav Goldberg, and Ido Dagan|[Link](pyatkin2021.md)|

## Parsing
|Paper|Authors|Docs|
|-|-|-|
|[Multilingual Constituency Parsing with Self-Attention and Pre-Training](https://arxiv.org/abs/1812.11760)|Nikita Kitaev, Steven Cao, and Dan Klein|[Link](kitaev2019.md)|

## Others
|Paper|Authors|Docs|
|-|-|-|
|[RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text](https://arxiv.org/abs/2010.03070)|Liam Dugan, Daphne Ippolito, Arun Kirubarajan, and Chris Callison-Burch|[Link](dugan2020.md)|
|[Learning to Capitalize with Character-Level Recurrent Neural Networks: An Empirical Study](https://aclanthology.org/D16-1225/)|Raymond Hendy Susanto, Hai Leong Chieu, and Wei Lu|[Link](susanto2016.md)|
|[MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics](https://arxiv.org/abs/2010.03636)|Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner|[Link](chen2020.md)|
|[Large-Scale QA-SRL Parsing](https://arxiv.org/abs/1805.05377)|Nicholas FitzGerald, Julian Michael, Luheng He, and Luke Zettlemoyer|[Link](fitzgerald2018.md)|
